{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWwrfbbVGOA"
   },
   "source": [
    "#### [LangChain Handbook](https://qdrant.tech/articles/langchain-integration/)\n",
    "\n",
    "# LangChain Retrieval Agent\n",
    "\n",
    "`Conversational agents` although being very accurate, face some issues with data freshness, accessing internal documentations and knowledge about specific domains. On the other hand `retrieval augmentation` solves these issues but then it would always retrieve on every query which is inefficient in most of simple cases in which retrieval is not required. Using both of these methods simultaneously gives us a system which can answer simple questions directly and seek for extra knowledge when queried with complex questions. We will see how to do so with LangChain and Qdrant in this notebook with Falcon-7B-Instruct as our LLM. Falcon-7B-Instruct is a ready-to-use chat/instruct model based on Falcon-7B which outperforms comparable open-source models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "Let's get started by installing the packages needed for notebook to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pva9ehKXUpU2",
    "outputId": "3bbcf2dd-1889-412f-d45a-f56945ac4f2f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU qdrant-client==1.3.1 langchain==0.0.235 datasets==2.13.1 sentence_transformers==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\karti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\karti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import qdrant_client\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTgrOQziXUto"
   },
   "source": [
    "## Building the Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNyRsz0ZXXaq"
   },
   "source": [
    "Our knowledge base will be prepared from a dataset from Hugging Face called `vietgpt/multi_news_en`, it consists of about 45k records of news articles and human-written summaries of these articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laSDMjqQXuj-",
    "outputId": "5272df99-eb4b-4ec2-c513-504e067be2b6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/karti/.cache/huggingface/datasets/vietgpt___parquet/vietgpt--multi_news_en-4921e62a5a375465/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary'],\n",
       "    num_rows: 44972\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"vietgpt/multi_news_en\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8casaLEpX18U"
   },
   "source": [
    "We convert the dataset into a pandas dataframe for further use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "JnWZTcJiXzor",
    "outputId": "42659cec-23c3-4349-b007-676da99d834d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>National Archives \\n \\n Yes, it’s that time ag...</td>\n",
       "      <td>– The unemployment rate dropped to 8.2% last m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOS ANGELES (AP) — In her first interview sinc...</td>\n",
       "      <td>– Shelly Sterling plans \"eventually\" to divorc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GAITHERSBURG, Md. (AP) — A small, private jet ...</td>\n",
       "      <td>– A twin-engine Embraer jet that the FAA descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tucker Carlson Exposes His Own Sexism on Twitt...</td>\n",
       "      <td>– Tucker Carlson is in deep doodoo with conser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man accused of removing another man's testic...</td>\n",
       "      <td>– What are the three most horrifying words in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  National Archives \\n \\n Yes, it’s that time ag...   \n",
       "1  LOS ANGELES (AP) — In her first interview sinc...   \n",
       "2  GAITHERSBURG, Md. (AP) — A small, private jet ...   \n",
       "3  Tucker Carlson Exposes His Own Sexism on Twitt...   \n",
       "4  A man accused of removing another man's testic...   \n",
       "\n",
       "                                             summary  \n",
       "0  – The unemployment rate dropped to 8.2% last m...  \n",
       "1  – Shelly Sterling plans \"eventually\" to divorc...  \n",
       "2  – A twin-engine Embraer jet that the FAA descr...  \n",
       "3  – Tucker Carlson is in deep doodoo with conser...  \n",
       "4  – What are the three most horrifying words in ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.to_pandas()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2_Pt7N6Zg2X"
   },
   "source": [
    "### Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGoS84KYZnSK"
   },
   "source": [
    "We will use the `all-MiniLM-L6-v2`, which is used to create vector representations of our records and also for our search queries. These vector embeddings capture the semantic meaning of the documents or records. Then, during the retrieval phase, similarity measure (i.e., cosine similarity) is applied in vector space to find the most similar records to a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U57x2_87YSpb",
    "outputId": "cf4c99af-24b7-4bf5-97ea-71091c8fc2ce",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: C:\\Users\\karti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary C:\\Users\\karti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set device to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device != \"cuda\":\n",
    "    print(\n",
    "        f\"You are using {device}. This is much slower than using \"\n",
    "        \"a CUDA-enabled GPU. If on Colab you can change this by \"\n",
    "        \"clicking Runtime > Change runtime type > GPU.\"\n",
    "    )\n",
    "# Instantiate the SentenceTransformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQTfOTR6aBRS"
   },
   "source": [
    "## Initialize Qdrant client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3wrG-9yaJel",
    "outputId": "842bf46f-fd0f-4322-8d90-a7fd769b7687",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[]\n",
      "collections=[CollectionDescription(name='langchain-retrieval-agent')]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Qdrant client\n",
    "\n",
    "current_folder = Path.cwd()  # Get the current folder\n",
    "qdrant_folder = current_folder / \"qdrant\"\n",
    "qdrant_folder.mkdir()  # Create qdrant folder to store collection\n",
    "\n",
    "client = QdrantClient(path=qdrant_folder.resolve())  # path to new qdrant folder\n",
    "\n",
    "collection_name = \"langchain-retrieval-agent\"\n",
    "\n",
    "collections = client.get_collections()\n",
    "print(collections)\n",
    "\n",
    "# only create collection if it doesn't exist\n",
    "if collection_name not in collections:\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=384,  # specifying dimensionality of vectors output by model\n",
    "            distance=models.Distance.COSINE,  # specifying which metric will be used to check similarity of vectors\n",
    "        ),\n",
    "    )\n",
    "collections = client.get_collections()\n",
    "print(collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD5IGOoLaVx7"
   },
   "source": [
    "## Generate Embeddings -> Store in Qdrant\n",
    "Now we will generate embeddings for our summary column. We will do so in batches which is much faster than doing it individually. And then send a single api call to upsert the batch (also much faster).\n",
    "\n",
    "In qdrant, we need an id (a unique value), embedding (embeddings for the summary column), and metadata for each document in the dataset. The metadata is a dictionary containing data relevant to our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "321ebcd192fc41e0a7a2ba9c0e7b8556",
      "7c898f43b39545879013df760aa4f6f1",
      "1b5fe655986842a6afd4a2eb2c0f47fc",
      "76872ca737b64616aeacc90f2f7fc655",
      "0679733cce4f4b5eb25f8f56dc7ebe22",
      "71551fc297b54fb4a21d787d117e01a1",
      "b90283522431447b925b226431bb4fa6",
      "a03d372e8b4f456e8663e54c818e0fe9",
      "6fc773ccbc594fddbc1f2f0adaa96186",
      "07b47869f727455fb5ba67e99767c04c",
      "4b2e12a33584489690aaf69f51cd4a3b"
     ]
    },
    "id": "AhDcbRGTaWPi",
    "outputId": "dca5a4d0-5d19-4542-c26c-3073bfa13f2a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c4d672be6b4d848e05e2600645ce83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector count in collection: 44972\n",
      "CPU times: total: 4min 51s\n",
      "Wall time: 10min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 1024  # specify batch size according to your RAM and compute, higher batch size = more RAM usage\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i + batch_size)  # get end of batch\n",
    "    batch = data.iloc[i:i_end]  # extract batch\n",
    "    meta = batch.to_dict(orient=\"records\")  # first get metadata fields for this record\n",
    "    embeds = model.encode(\n",
    "        batch[\"summary\"].tolist()\n",
    "    ).tolist()  # encoding the whole batch of summary passages into vectors\n",
    "\n",
    "    ids = list(range(i, i_end))  # create unique IDs\n",
    "\n",
    "    # upsert to qdrant\n",
    "    client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=models.Batch(ids=ids, vectors=embeds, payloads=meta),\n",
    "    )\n",
    "\n",
    "collection_vector_count = client.get_collection(\n",
    "    collection_name=collection_name\n",
    ").vectors_count\n",
    "print(f\"Vector count in collection: {collection_vector_count}\")\n",
    "assert collection_vector_count == len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDUnLdy1b7G1"
   },
   "source": [
    "Let's check our collection info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SiccGZKAb_Qo",
    "outputId": "5cafd8c8-771f-46d0-e261-12c9b5b1b058",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=44972, indexed_vectors_count=0, points_count=44972, segments_count=1, config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=None, replication_factor=None, write_consistency_factor=None, on_disk_payload=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=None, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None), payload_schema={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_collection(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-3oolT5cCR8"
   },
   "source": [
    "## Creating a Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcZ12U06cCH5"
   },
   "source": [
    "We will reuse the same collection to create a vector store of langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0MBJ477-cFNw",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.qdrant.Qdrant at 0x13e2e1748d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"),\n",
    "    content_payload_key=\"summary\",\n",
    ")\n",
    "qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying\n",
    "Now with the help of langchain we can directly do `similarity search`(without generation component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='– President Trump accused the \"very, very dishonest press\" Monday of not wanting to report ISIS attacks in Europe. \"They have their reasons, you understand that,\" he said, speaking to troops at MacDill Air Force Base in Florida, the headquarters of US Central Command. The White House later released a list of 78 terror attacks it claimed did not get the media attention they deserved. The incidents, however, included extensively reported attacks like the San Bernardino mass shooting, the Orlando nightclub attack, and the Paris Bataclan attack, reports NPR, which has the list in full. A roundup of coverage: Sean Spicer said Monday that Trump really meant that the incidents were underreported, not ignored, CBS News reports. \"He felt that members of media don\\'t always cover some of those events to the extent that other events might get covered,\" Spicer said. \"Like a protest gets blown out of the water, and yet an attack or a foiled attack doesn\\'t necessarily get the same coverage.\" The list—released with spelling mistakes, including the repeated use of \"attak\" and \"attaks\"—included little-known incidents in which nobody was killed and incidents with no clear link to ISIS, as well as major attacks that dominated media coverage for weeks, reports the Australian Broadcasting Corporation, which notes that one of the five Australian incidents on the list, a fatal youth-hostel stabbing, was determined to be a murder and not a terrorist attack. \"All over Europe it\\'s happening. It\\'s gotten to a point where it\\'s not even being reported,\" Trump said, though the AP notes that out of the 78 listed attacks the White House says were \"executed or inspired by\" ISIS since September 2014, fewer than half occurred in Europe. \"ISIS is on a campaign of genocide, committing atrocities across the world,\" Trump told troops, according to the White House transcript of his remarks. After the list was released, White House spokeswoman Lindsay Walters said the \"real point\" was that attacks are no longer getting the \"wall-to-wall\" coverage they did a few years ago. \"They\\'re happening so often—at a rate of more than once every two weeks, according to the list we sent around—that networks are not devoting to each of them the same level of coverage they once did,\" she said, per the Washington Post. The Guardian reports that political analyst David Gergen described the \"outrageous\" claim that the media didn\\'t want the public to know about terrorism as another example of Trump engaging in falsehoods \"without producing any serious evidence.\" \"The list includes San Bernardino, as if the press didn\\'t cover that sufficiently,\" Gergen told CNN. \"It\\'s just astonishing and it\\'s beneath the dignity of the presidency,\" he noted, warning that this is \"the way democracies come unraveled.\"', metadata={}),\n",
       " Document(page_content='– Since 9/11, the NYPD has stopped 14 terror attacks, right? It must be true because Mayor Bloomberg and police chief Ray Kelly keeping trumpeting the stat, and the media keep circulating it. (Like in this profile.) ProPublica took a look at the NYPD\\'s own list of the 14 to test the accuracy of the claim. \"Is it true? In a word, no,\" writes Justin Elliott. The boast \"overstates both the number of serious, developed terrorist plots against New York and exaggerates the NYPD\\'s role in stopping attacks.\" Of the 14, ProPublica says two, maybe three, qualify as true terror threats. And that includes \"a failed attempt to bomb Times Square by a Pakistani-American in 2010 that the NYPD did not stop.\" What\\'s more, the NYPD doesn\\'t seem to have played a big role in most of the busts. \"In several cases, it played no role at all.\" See the full article and a breakdown of the 14 cases here. (Asked about the story today, Bloomberg responded: “I could make as cogent an argument there’s double or triple the number that were stopped. We just don’t know about it.\" More on that here.)', metadata={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"When did the biggest terror attack on USA happen?\"\n",
    "qdrant.similarity_search(query, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zGF6YsgczqT"
   },
   "source": [
    "Looks like we're getting good results. Let's take a look at how we can begin integrating this into a conversational chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFsIOm73dcOI",
    "tags": []
   },
   "source": [
    "## Initializing the Conversational Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMv6TXWkdfNR"
   },
   "source": [
    "We will use `Falcon-7B-Instruct` as our LLM, we will also need `conversational memory` to store previous conversations and a `ConversationalRetrievalChain` chain to retrieve extra data when needed.\n",
    "\n",
    "We will use inference api of Falcon-7B-Instruct from hugging face to query. We need an API_TOKEN to do so which we can get from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get API_TOKEN from huggingface website\n",
    "API_TOKEN = os.getenv(\"API_TOKEN\") or \"API_TOKEN\"\n",
    "if not API_TOKEN:\n",
    "    raise ValueError(\n",
    "        \"API_TOKEN is not set. Please obtain a valid API_TOKEN from the Hugging Face website.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zMRs9Klic5-Y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat completion llm\n",
    "llm = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=API_TOKEN,\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_new_tokens\": 2000,  # maximum number of tokens the model will generate\n",
    "    },\n",
    ")\n",
    "# conversational memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# conversational retrieval qa chain using vector store\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, qdrant.as_retriever(), memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate answer to our query we will use the `query` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer to the given question using the ConversationalRetrievalChain which uses\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to generate an answer for.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer.\n",
    "    \"\"\"\n",
    "    result = qa({\"question\": question})\n",
    "    return result[\"answer\"][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the components are ready. We can start querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The largest GDP in 2020 was the United States with a GDP of 21.44 trillion USD.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"What was the largest gdp in 2020?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Michael Phelps\\n\\nAnswer:  Michael Phelps\\n\\nExplanation:  Michael Phelps has won a total of 21 Olympic gold medals, making him the most decorated Olympian in history.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"Which person has won the most olympic medals in history?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Michael Phelps is a swimmer.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"What sport did he used to play?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWivmw9F3bCw"
   },
   "source": [
    "We are getting the answers in the way we wanted. The agent can refer to previous conversation as a source of information.\n",
    "That's all we wanted to showcase. You can do more queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa1whr8V3Wfm"
   },
   "outputs": [],
   "source": [
    "client.delete_collection(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ykg5TYA033yR"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0679733cce4f4b5eb25f8f56dc7ebe22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07b47869f727455fb5ba67e99767c04c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b5fe655986842a6afd4a2eb2c0f47fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a03d372e8b4f456e8663e54c818e0fe9",
      "max": 189,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6fc773ccbc594fddbc1f2f0adaa96186",
      "value": 189
     }
    },
    "321ebcd192fc41e0a7a2ba9c0e7b8556": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7c898f43b39545879013df760aa4f6f1",
       "IPY_MODEL_1b5fe655986842a6afd4a2eb2c0f47fc",
       "IPY_MODEL_76872ca737b64616aeacc90f2f7fc655"
      ],
      "layout": "IPY_MODEL_0679733cce4f4b5eb25f8f56dc7ebe22"
     }
    },
    "4b2e12a33584489690aaf69f51cd4a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fc773ccbc594fddbc1f2f0adaa96186": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "71551fc297b54fb4a21d787d117e01a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76872ca737b64616aeacc90f2f7fc655": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07b47869f727455fb5ba67e99767c04c",
      "placeholder": "​",
      "style": "IPY_MODEL_4b2e12a33584489690aaf69f51cd4a3b",
      "value": " 189/189 [03:23&lt;00:00,  1.12s/it]"
     }
    },
    "7c898f43b39545879013df760aa4f6f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71551fc297b54fb4a21d787d117e01a1",
      "placeholder": "​",
      "style": "IPY_MODEL_b90283522431447b925b226431bb4fa6",
      "value": "100%"
     }
    },
    "a03d372e8b4f456e8663e54c818e0fe9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b90283522431447b925b226431bb4fa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
