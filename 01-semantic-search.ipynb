{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W390nLBmZlId"
   },
   "source": [
    "\n",
    "# Semantic Search\n",
    "\n",
    "\n",
    "In this walkthrough we will see how to use Qdrant for semantic search. To begin we must install the required prerequisite libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q03L1BYEZQfe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  datasets==2.12.0 \\\n",
    "  sentence-transformers==2.2.2 \\\n",
    "  qdrant-client==1.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\karti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\karti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrSfFiIC5roI"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kujS_e8s55oJ"
   },
   "source": [
    "The dataset preparation process requires a few steps:\n",
    "\n",
    "1. We download the squad(Stanford Question Answering Dataset) dataset from Hugging Face Datasets.\n",
    "\n",
    "2. The question content of the dataset is embedded into vectors.\n",
    "\n",
    "3. We reformat into a `(id, vector, payload)` structure. Points are a central entity that Qdrant operates with. They contain records consisting of a vector, an id, and payload.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeJPWu9P7EtR",
    "outputId": "2323c6b2-5feb-4601-e843-1dc04e272008",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/karti/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\", split=\"train[0:87000]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngFHND1nQQU2"
   },
   "source": [
    "The dataset contains ~87K questions posed by crowdworkers on a set of Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'What is in front of the Notre Dame Main Building?',\n",
       " 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?',\n",
       " 'What is the Grotto at Notre Dame?',\n",
       " 'What sits on top of the Main Building at Notre Dame?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing starting 5 question content from dataset\n",
    "dataset[:5][\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_Zy8zoeQmRZ"
   },
   "source": [
    "We can extract all questions into a single `questions` list and remove duplicates as we do not want same repeated query results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Store questions in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heGUpy_37Eis",
    "outputId": "a9b61af4-6595-44fc-bd35-7a7501b26123",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 unique questions--\n",
      "Do all regions perceive that term \"black people\" the same?\n",
      "Who is the director of the Genome Center at Washington University?\n",
      "Who argued in 2003 that all clades are by definition monophyletic groups?\n",
      "In which color model is green one of the additive primary colors?\n",
      "How long did the Hollywood round air for in season eight of American Idol?\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the questions\n",
    "questions = []\n",
    "\n",
    "# Iterate over each record in the dataset\n",
    "for record in dataset:\n",
    "    question = record[\"question\"]  # Extract the question from the current record\n",
    "    questions.append(record[\"question\"])  # Append the question to the list of questions\n",
    "\n",
    "# remove duplicates to eliminate redundancy and ensure that each question is unique\n",
    "questions = list(set(questions))\n",
    "\n",
    "# Print the first 5 unique questions using '\\n' as a separator\n",
    "print(\"First 5 unique questions--\")\n",
    "print(\"\\n\".join(questions[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BknpfucRkkm"
   },
   "source": [
    "With our questions ready to go we can move on to demoing steps **2** and **3** above.\n",
    "\n",
    "\n",
    "To create our embeddings we will use the `MiniLM-L6` sentence transformer model. This is a very efficient semantic similarity embedding model from the `sentence-transformers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate SentenceTransformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxcGjb9GSEqA",
    "outputId": "e3b269d0-c7f4-41d8-85ad-5a94a0b79d8d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a CUDA-enabled GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device != \"cuda\":\n",
    "    print(\n",
    "        f\"You are using {device}. This is much slower than using \"\n",
    "        \"a CUDA-enabled GPU. If on Colab you can change this by \"\n",
    "        \"clicking Runtime > Change runtime type > GPU.\"\n",
    "    )\n",
    "\n",
    "# Instantiate the SentenceTransformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy2itPb0S5js"
   },
   "source": [
    "There are *three* interesting bits of information in the above model printout. Those are:\n",
    "\n",
    "* `max_seq_length` is `256`. That means that the maximum number of tokens (like words) that can be encoded into a single vector embedding is `256`. Anything beyond this *must* be truncated.\n",
    "\n",
    "* `word_embedding_dimension` is `384`. This number is the dimensionality of vectors output by this model. It is important that we know this number later when initializing our Qdrant vector collection.\n",
    "\n",
    "* `Normalize()`. This final normalization step indicates that all vectors produced by the model are normalized. That means that models that we would typical measure similarity for using *cosine similarity* can also make use of the *dotproduct* similarity metric. In fact, with normalized vectors *cosine* and *dotproduct* are equivalent.\n",
    "\n",
    "Moving on, we can create a sentence embedding using this model like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dyzoJEsAULOe",
    "outputId": "a1ea6149-4b2d-4dfb-e984-b067fb9980d5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example on how to encode query\n",
    "query = \"which city is the most populated in the world?\"\n",
    "\n",
    "# encoding the query into a vector\n",
    "encoded_query = model.encode(query)\n",
    "\n",
    "# getting the dimensionality of the vector\n",
    "encoded_query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVZi8xevUWM6"
   },
   "source": [
    "Encoding this single sentence leaves us with a `384` dimensional sentence embedding (aligned to the `word_embedding_dimension` above).\n",
    "\n",
    "To prepare this for `upsert` to Qdrant, all we do is this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Upsert Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T38HdqxwVg6p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unique identifier for the vector\n",
    "_id = \"0\"\n",
    "\n",
    "# encoding the query into a vector\n",
    "encoded_query = model.encode(query)\n",
    "\n",
    "# Additional payload associated with the vector\n",
    "payload = {\"question\": query}\n",
    "\n",
    "# List of tuples representing the points to be upserted\n",
    "points = [(_id, encoded_query, payload)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiXig_rHV2Wz"
   },
   "source": [
    "Later when we do upsert our data to Qdrant, we will be doing so by using Batch uploading process from our models module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebd7XSamfMsC"
   },
   "source": [
    "We begin by initializing our connection to Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Qdrant client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mc66NEBAcQHY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Qdrant client\n",
    "client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdaTip6CfllN"
   },
   "source": [
    "Now we create a new collection called `semantic-search`. It's important that we align the collection `dimension` and `metric` parameters with those required by the `MiniLM-L6` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Collection\n",
    "\n",
    "Now the data is ready, we can set up our collection to store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "p1pyQh8gfm2-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[]\n",
      "collections=[CollectionDescription(name='semantic-search')]\n"
     ]
    }
   ],
   "source": [
    "question_collection = \"semantic-search\"\n",
    "\n",
    "collections = client.get_collections()\n",
    "print(collections)\n",
    "\n",
    "# only create collection if it doesn't exist\n",
    "if question_collection not in collections:\n",
    "    client.recreate_collection(\n",
    "        collection_name=question_collection,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=model.get_sentence_embedding_dimension(),  # specifying dimensionality of vectors output by model\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    )\n",
    "collections = client.get_collections()\n",
    "print(collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUd1VGg6i108"
   },
   "source": [
    "Now we upsert the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upserting vectors\n",
    "\n",
    "Create an index and payload for your vectors.\n",
    "\n",
    "Note: Qdrant can only take in native Python iterables like lists and tuples. This is why you will notice the .tolist() method attached to our data matrix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "0e5dc9a271184100a92cd6b373ab8e7d",
      "8396bfd36c2b4869a0b4680587b604f1",
      "d714f8b3ebca4195b747ab18d196b88b",
      "56266bc7062541f5ba042848205270ef",
      "78fab28034dd428c9f49a5653fd003e5",
      "d8ca1c42783f41a8bd17491073883fdd",
      "cffd5e66b82344b0946630eb320ca5b4",
      "b63c0eee680f4708a4c68a4746cb21a4",
      "12e8c994f3ac4f88ae5387c72b4d8cc7",
      "9ea70e9ea5fb4040a080b941950b0fef",
      "b30a98ddfc1841e381937571edcfae71"
     ]
    },
    "id": "RhR6WOi1huXZ",
    "outputId": "ef9f74ef-2ae3-4eb3-cef4-6814e98861a7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 156.37646007537842 seconds\n",
      "vector count in collection-  86764\n"
     ]
    }
   ],
   "source": [
    "# Start the timer to measure total time elapsed.\n",
    "start_time = time.time()\n",
    "\n",
    "ids = list(range(len(questions)))  # creating index for vectors\n",
    "encoded_queries = model.encode(questions)  # encoding the questions into vectors\n",
    "\n",
    "payload = []\n",
    "for i in range(len(questions)):  # question as payload\n",
    "    payload.append({\"question\": questions[i]})\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=question_collection,\n",
    "    points=models.Batch(ids=ids, vectors=encoded_queries.tolist(), payloads=payload),\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "print(\n",
    "    \"vector count in collection- \",\n",
    "    client.get_collection(collection_name=question_collection).vectors_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrK_IN079Vuu"
   },
   "source": [
    "## Making Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rr4unPAq9alb"
   },
   "source": [
    "Now that our collection is populated we can begin making queries. We are performing a semantic search for *similar questions*, so we should embed and search with another question. Let's begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWcO7jAK-N_1",
    "outputId": "db437488-c928-4fd7-e4b7-83c93ac67f88",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Results--\n",
      "• 1: id=49359 version=0 score=0.7751039089283264 payload={'question': 'Which inventors patented the tungsten filament lamp?'} vector=None\n",
      "• 2: id=12305 version=0 score=0.7499778499609704 payload={'question': 'How many inventors came up with electric lamps before Thomas Edison?'} vector=None\n",
      "• 3: id=23455 version=0 score=0.7359869975580777 payload={'question': 'Who patented an incandescent light bulb in Russia in 1874?'} vector=None\n",
      "• 4: id=27396 version=0 score=0.7244679924060471 payload={'question': 'What company invented the tantalum light filament?'} vector=None\n",
      "• 5: id=49122 version=0 score=0.6791491146434465 payload={'question': 'Who first patented a method to produce high-brightness blue LEDs?'} vector=None\n"
     ]
    }
   ],
   "source": [
    "query = \"who is the inventor of light bulb?\"\n",
    "# create the query vector\n",
    "encoded_query = model.encode(query).tolist()\n",
    "\n",
    "\n",
    "# now query\n",
    "def search(encoded_query):\n",
    "    try:\n",
    "        return client.search(\n",
    "            collection_name=question_collection,\n",
    "            query_vector=encoded_query,\n",
    "            limit=5,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print({e})\n",
    "        return []\n",
    "\n",
    "\n",
    "query_results = search(encoded_query)\n",
    "if len(query_results) == 0:\n",
    "    print(\"No results found. Check the client search arguments and collection upserts.\")\n",
    "else:\n",
    "    # printing the 5 results of the query\n",
    "    print(\"Top 5 Results--\")\n",
    "    for i, result in enumerate(query_results):\n",
    "        print(f\"• {i+1}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XwOWcgo_QtI"
   },
   "source": [
    "In the returned response `query_results` we can see the most relevant questions to our particular query. We can reformat this response to be a little easier to read:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting scores of query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gy7isg_f-vWg",
    "outputId": "bbeee182-8e31-4bba-da39-38521b13683f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S.no. Score   Similar Questions\n",
      "• 1 : 0.78--> Which inventors patented the tungsten filament lamp?\n",
      "• 2 : 0.75--> How many inventors came up with electric lamps before Thomas Edison?\n",
      "• 3 : 0.74--> Who patented an incandescent light bulb in Russia in 1874?\n",
      "• 4 : 0.72--> What company invented the tantalum light filament?\n",
      "• 5 : 0.68--> Who first patented a method to produce high-brightness blue LEDs?\n"
     ]
    }
   ],
   "source": [
    "print(\"S.no.\" + \" \" * 1 + \"Score   Similar Questions\")\n",
    "# printing only the scores and questions of the 5 results\n",
    "for i, result in enumerate(query_results):\n",
    "    print(f\"• {i+1} : {round(result.score, 2)}--> {result.payload['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JK5yApl_5fE"
   },
   "source": [
    "These are good results, let's try and modify the words being used to see if we still surface similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying query, keeping the sentiment same and checking the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJbjE-iq_yMr",
    "outputId": "b81c67b1-9fe7-48c3-d2b0-14100cbbc25d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S.no. Score    Similar Questions\n",
      "• 1 : 0.71 --> Which inventors patented the tungsten filament lamp?\n",
      "• 2 : 0.7 --> How many inventors came up with electric lamps before Thomas Edison?\n",
      "• 3 : 0.67 --> Who patented an incandescent light bulb in Russia in 1874?\n",
      "• 4 : 0.63 --> Who first patented a method to produce high-brightness blue LEDs?\n",
      "• 5 : 0.62 --> What company invented the tantalum light filament?\n"
     ]
    }
   ],
   "source": [
    "query = \"which person is accredited with the invention of light bulb?\"\n",
    "encoded_query = model.encode(query).tolist()\n",
    "\n",
    "# now query\n",
    "query_results = search(encoded_query)\n",
    "\n",
    "if len(query_results) == 0:\n",
    "    print(\"no results found,Check the client search arguments and collection upserts.\")\n",
    "else:\n",
    "    print(\"S.no.\" + \" \" * 1 + \"Score    Similar Questions\")\n",
    "    for i, result in enumerate(query_results):\n",
    "        print(f\"• {i+1} : {round(result.score, 2)} --> {result.payload['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIAxOPb-A2w_"
   },
   "source": [
    "Here we used different terms in our query than that of the returned documents. \n",
    "\n",
    "Despite these very different terms and *lack* of term overlap between query and returned documents — we get highly relevant results — this is the power of *semantic search*.\n",
    "\n",
    "You can go ahead and ask more questions above. When you're done, delete the collection to save resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Delete collection to save resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-cWdeKzhAtww",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_collection(collection_name=question_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMMJSu_DbRx0"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e5dc9a271184100a92cd6b373ab8e7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8396bfd36c2b4869a0b4680587b604f1",
       "IPY_MODEL_d714f8b3ebca4195b747ab18d196b88b",
       "IPY_MODEL_56266bc7062541f5ba042848205270ef"
      ],
      "layout": "IPY_MODEL_78fab28034dd428c9f49a5653fd003e5"
     }
    },
    "12e8c994f3ac4f88ae5387c72b4d8cc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "56266bc7062541f5ba042848205270ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ea70e9ea5fb4040a080b941950b0fef",
      "placeholder": "​",
      "style": "IPY_MODEL_b30a98ddfc1841e381937571edcfae71",
      "value": " 1063/1063 [02:48&lt;00:00,  6.41it/s]"
     }
    },
    "78fab28034dd428c9f49a5653fd003e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8396bfd36c2b4869a0b4680587b604f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8ca1c42783f41a8bd17491073883fdd",
      "placeholder": "​",
      "style": "IPY_MODEL_cffd5e66b82344b0946630eb320ca5b4",
      "value": "100%"
     }
    },
    "9ea70e9ea5fb4040a080b941950b0fef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b30a98ddfc1841e381937571edcfae71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b63c0eee680f4708a4c68a4746cb21a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cffd5e66b82344b0946630eb320ca5b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d714f8b3ebca4195b747ab18d196b88b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b63c0eee680f4708a4c68a4746cb21a4",
      "max": 1063,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_12e8c994f3ac4f88ae5387c72b4d8cc7",
      "value": 1063
     }
    },
    "d8ca1c42783f41a8bd17491073883fdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
